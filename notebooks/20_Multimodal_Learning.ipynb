{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "# Multimodal Learning\n",
    "\n",
    "⸻\n",
    "\n",
    "1. What Is Multimodal Learning?\n",
    "\n",
    "Multimodal learning combines information from multiple data modalities, such as:\n",
    "\t•\tText (e.g., captions, documents)\n",
    "\t•\tImage (e.g., pixels, features)\n",
    "\t•\tAudio (e.g., speech, sounds)\n",
    "\t•\tVideo, sensor, or tabular data\n",
    "\n",
    "Goal: Learn joint representations or complementary features for better prediction, classification, or generation.\n",
    "\n",
    "⸻\n",
    "\n",
    "2. Fusion Strategies\n",
    "\n",
    "Early Fusion:\n",
    "\t•\tCombine raw features (e.g., concatenate vectors)\n",
    "\t•\tExample: [text features | image features]\n",
    "\n",
    "Late Fusion:\n",
    "\t•\tTrain separate models for each modality\n",
    "\t•\tCombine decisions (e.g., ensemble of outputs)\n",
    "\n",
    "Hybrid Fusion:\n",
    "\t•\tIntermediate representations are combined\n",
    "\t•\tOften seen in attention-based models (e.g., CLIP, ViLBERT)\n",
    "\n",
    "⸻\n",
    "\n",
    "3. Applications\n",
    "\n",
    "Modality Pair\tTask Example\n",
    "Text + Image\tImage Captioning, VQA\n",
    "Text + Audio\tSpeech Recognition\n",
    "Video + Audio\tEmotion Recognition\n",
    "Image + Sensor\tMedical Diagnosis\n",
    "Text + Table\tFinancial Report Analysis\n",
    "\n",
    "\n",
    "\n",
    "⸻\n",
    "\n",
    "4. Architecture Example (Text + Image)\n",
    "\n",
    "Image → CNN → Img Features ┐\n",
    "                            ├→ Concatenate → MLP → Output\n",
    "Text  → BERT → Txt Features ┘\n",
    "\n",
    "Or using attention:\n",
    "\n",
    "Image → CNN → Keys/Values ┐\n",
    "                           ├→ Cross Attention\n",
    "Text → BERT → Queries     ┘\n",
    "\n",
    "\n",
    "\n",
    "⸻\n",
    "\n",
    "5. Python: Simple Text + Image Fusion (Early Fusion)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "class MultimodalClassifier(nn.Module):\n",
    "    def __init__(self, txt_dim=768, img_dim=2048, hidden=512, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.img_fc = nn.Linear(img_dim, txt_dim)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(txt_dim + txt_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, text_input_ids, text_attention_mask, img_features):\n",
    "        txt_out = self.bert(input_ids=text_input_ids, attention_mask=text_attention_mask)\n",
    "        txt_vec = txt_out.pooler_output  # [B, 768]\n",
    "        img_vec = self.img_fc(img_features)  # [B, 768]\n",
    "        x = torch.cat([txt_vec, img_vec], dim=1)  # [B, 1536]\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "\n",
    "⸻\n",
    "\n",
    "6. Challenges in Multimodal Learning\n",
    "\t•\tMissing modalities: Not all inputs always available\n",
    "\t•\tAlignment: Matching time/space between modalities\n",
    "\t•\tNoise: One modality may dominate or be noisy\n",
    "\t•\tDimensional mismatch: Vectors of vastly different sizes\n",
    "\n",
    "⸻\n",
    "\n",
    "7. Large Multimodal Models\n",
    "\n",
    "Model\tModality\tTask\n",
    "CLIP\tText + Image\tRetrieval, Zero-shot vision\n",
    "Flamingo\tText + Image\tVQA, Captioning\n",
    "Whisper\tAudio + Text\tTranscription, Translation\n",
    "Gemini/GPT-4V\tText + Image/Code\tMultimodal reasoning\n",
    "\n",
    "\n",
    "\n",
    "⸻\n",
    "\n",
    "Would you like an example of Multimodal Contrastive Learning like CLIP, or Vision + Sensor fusion for time-series prediction?"
   ],
   "id": "b03adb7cb24dd2b2"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import nbformat\n",
    "import yaml\n",
    "\n",
    "# Path to your _toc.yml\n",
    "TOC_FILE = \"_toc.yml\"\n",
    "\n",
    "# Where to create notebooks (can be same as where _toc.yml is)\n",
    "OUTPUT_DIR = \".\"\n",
    "\n",
    "# Template notebook content\n",
    "def make_notebook(title):\n",
    "    nb = nbformat.v4.new_notebook()\n",
    "    nb[\"cells\"] = [\n",
    "        nbformat.v4.new_markdown_cell(f\"# {title}\"),\n",
    "        nbformat.v4.new_code_cell(\"# Your code here\")\n",
    "    ]\n",
    "    return nb\n",
    "\n",
    "def create_notebook(path, title):\n",
    "    \"\"\"Create an .ipynb file if it doesn't exist.\"\"\"\n",
    "    if not path.endswith(\".ipynb\"):\n",
    "        path += \".ipynb\"\n",
    "    full_path = os.path.join(OUTPUT_DIR, path)\n",
    "    os.makedirs(os.path.dirname(full_path), exist_ok=True)\n",
    "    if not os.path.exists(full_path):\n",
    "        nb = make_notebook(title)\n",
    "        with open(full_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            nbformat.write(nb, f)\n",
    "        print(f\"Created: {full_path}\")\n",
    "    else:\n",
    "        print(f\"Exists:  {full_path}\")\n",
    "\n",
    "def process_entry(entry):\n",
    "    \"\"\"Recursively process toc entries.\"\"\"\n",
    "    if isinstance(entry, dict):\n",
    "        if \"file\" in entry:\n",
    "            title = entry.get(\"title\", entry[\"file\"])\n",
    "            create_notebook(entry[\"file\"], title)\n",
    "        if \"sections\" in entry:\n",
    "            for sec in entry[\"sections\"]:\n",
    "                process_entry(sec)\n",
    "    elif isinstance(entry, list):\n",
    "        for e in entry:\n",
    "            process_entry(e)\n",
    "\n",
    "def main():\n",
    "    with open(TOC_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        toc = yaml.safe_load(f)\n",
    "\n",
    "    if \"root\" in toc:\n",
    "        create_notebook(toc[\"root\"], toc.get(\"title\", toc[\"root\"]))\n",
    "\n",
    "    if \"chapters\" in toc:\n",
    "        process_entry(toc[\"chapters\"])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "e57b5675cf7640f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7c62553861a46f32"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
